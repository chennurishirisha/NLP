{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336b23aa",
   "metadata": {},
   "source": [
    "Importing Libraries along with our Data\n",
    "Expanding Contractions\n",
    "Language Detection\n",
    "Tokenization\n",
    "Converting all Characters to Lowercase\n",
    "Removing Punctuations\n",
    "Removing Stopwords\n",
    "Parts of Speech Tagging\n",
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec70ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import fasttext\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "plt.xticks(rotation=70)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6f687",
   "metadata": {},
   "source": [
    "Importing our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('indeed_scrape.csv') as f:\n",
    "    df = pd.read_csv(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972946bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rws = df.loc[:, ['rating', 'rating_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rws['no_contract'] = rws['rating_description'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rws['rating_description_str'] = [' '.join(map(str, l)) for l in rws['no_contract']]\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ec136",
   "metadata": {},
   "outputs": [],
   "source": [
    "English Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c9d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"lid.176.bin\" \n",
    "model = fasttext.load_model(pretrained_model)\n",
    "langs = []\n",
    "for sent in rws['rating_description_str']:\n",
    "    lang = model.predict(sent)[0]\n",
    "    langs.append(str(lang)[11:13])\n",
    "rws['langs'] = langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ed37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now all we have to do is remove any non-english reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f2820",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "Now that we have removed any non-English reviews let’s apply our tokenizer in order to split each individual word into a token. We will apply NLTK.word_tokenize() function to the “rating_description_str” column and create a new column named “tokenized”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "rws['tokenized'] = rws['rating_description_str'].apply(word_tokenize)\n",
    "rws.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723678af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all Characters to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f635c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rws['lower'] = rws['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = string.punctuation\n",
    "rws['no_punc'] = rws['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b91675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "rws['stopwords_removed'] = rws['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming vs Lemmatization\n",
    "rws['pos_tags'] = rws['stopwords_removed'].apply(nltk.tag.pos_tag)\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4eb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply NLTK’s word lemmatizer.\n",
    "#apply NLTK’s word lemmatizer.\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "rws['wordnet_pos'] = rws['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can apply NLTK’s word lemmatizer within our trusty list comprehension. Notice, the lemmatizer function requires two parameters the word and its tag (in wordnet form).\n",
    "wnl = WordNetLemmatizer()\n",
    "rws['lemmatized'] = rws['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Lastly, we are going to save this work into a csv file for further exploratory data analysis which you can read all about in my next blog.\n",
    "rws.to_csv('indeed_scrape_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc3738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b74ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54c048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4dc470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461156ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
